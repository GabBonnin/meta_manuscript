---
title: "Mental Health Evaluation through Text Analysis"
subtitle: "umbrella project documentation"
author:
  name: Gabriel Bonnin
  affiliations: Ruhr University Bochum, Germany
  email: gabriel.bonnin@ruhr-uni-bochum.de
  corresponding: true
bibliography: "references/META.bib"
csl: "references/apa-2.csl"
---

::: callout-note
![](reports/Logo_RUBRS.svg){width="180px"}

This project is supported by the [Ruhr University Bochum Research School](https://www.research-school.rub.de).
:::

# Abstract

Psychotherapy is one of the most effective treatments for mental health problems, but its success depends on accurate diagnostic assessments. Current assessment practices largely rely on standardized closed-ended scales that, while reliable, may fail to capture the complexity, context and individuality of patients' mental states. Advances in artificial intelligence (AI) and natural language processing (NLP) enable the measurement of psychological constructs through natural language, offering a promising complement to traditional assessment methods by leveraging patients’ own descriptions of their experiences.

While previous NLP-based mental health research has primarily focused on social media language, this project applies state-of-the-art large language models (LLMs) to open-ended intake data from a German outpatient psychotherapy clinic. Before therapy, patients describe the development, context, and perceived causes of their problems, as well as their current difficulties and therapy goals, in their own words. These texts are linked to key clinical measures, including diagnoses, symptom severity, functional impairment, and treatment outcomes, providing an ecologically valid resource for studying language-based assessment in real-world clinical settings.

The project comprises several complementary substudies. First, we examine whether patient language at intake reflects cross-sectional symptom severity and impairment and whether it provides incremental information beyond established self-report questionnaires. Second, we conduct a thematic analysis of patient responses to different open-ended prompts to characterize recurring themes. Third, we evaluate whether pre-therapy language predicts longitudinal treatment outcomes beyond baseline symptom measures.

Together, these studies aim to clarify how patient-generated language can be used for assessment, interpretation, and prognosis in psychotherapy. The findings are intended to inform the development of clinically meaningful, language-based assessment tools that support personalized care and contribute to the modernization of mental health evaluation.

::: {pagebreak}
:::

# Introduction

Mental health problems pose a significant global challenge, accounting for a considerable proportion of deaths and disability-adjusted life years [@WorldHealthOrganization2017]. Psychotherapy is an effective and sustainable intervention for reducing symptoms and improving quality of life [@wampold2015great; @chorpita2011evidence], but it’s success critically depends on accurate assessments [@Lutz2022; @Jensen-Doss2008].

Standardized closed-ended tools such as the Beck Depression Inventory-II [@Beck1996] rely on numerical scales [@Likert1932] to structure and standardize assessments and are widely used in clinical research and practice. While these methods have advanced replicability and reliability in psychological assessment, they can miss important individual differences by restricting responses to pre-defined categories, limiting the ability to capture the complexity of mental states [@Kjell2024].

Recent advances in AI, particularly transformer-based LLMs [@Vaswani2017], present promising solutions to these limitations [@Kjell2024]. LLMs excel in analyzing context-rich natural language with remarkable accuracy across diverse tasks [@devlin-etal-2019-bert]. Open-ended response formats, where patients describe their experiences in their own words, provide high-dimensional, context-rich information that remains underutilized in current assessment practices. Empirical studies highlight the potential of NLP-based analysis of open-ended responses, achieving moderate convergence with closed-ended rating scales using traditional NLP methods [@kjell2019semantic] and nearing theoretical upper limits of accuracy with LLMs [@Kjell2022]. Preliminary research also highlights their potential for predicting clinically significant outcomes, including suicide risk [@zirikly-etal-2019-clpsych; @mohammadi-etal-2019-clac-clpsych; @matero-etal-2019-suicide].

However, much of the existing literature relies on social media or non-clinical text data, raising concerns about ecological validity and clinical relevance. In contrast, open-ended patient responses are routinely collected in clinical settings as part of pre-therapy intake procedures but remain largely underused in empirical research. At the Mental Health Research and Treatment Center at Ruhr University Bochum, patients respond to multiple prompts addressing the development and context of their problems, perceived causes, social reactions, current difficulties, and therapy goals. These narratives are linked to structured diagnostic interviews, repeated symptom assessments, clinician ratings, and longitudinal outcome measures. This unique, large-scale, and longitudinal clinical dataset enables a comprehensive examination of patient language across multiple analytic perspectives.

Accordingly, the present project is organized into three complementary substudies. The first investigates whether pre-therapy language reflects cross-sectional symptom severity and clinician-rated impairment and whether language-based representations provide incremental information beyond standardized self-report questionnaires. The second focuses on the semantic content and structure of patient narratives, using question-specific analyses to identify recurring themes and selective response patterns in how patients conceptualize their mental health problems. The third evaluates the prognostic value of pre-therapy language by testing whether patient narratives predict treatment response and individualized goal attainment over time, beyond baseline symptom severity.

By integrating assessment, interpretive, and prognostic perspectives, this project aims to advance the clinical use of natural language in psychotherapy. Ultimately, the findings seek to support more nuanced, patient-centered assessment practices and contribute to the development of language-based tools that complement existing diagnostic frameworks and inform personalized treatment planning.

# Shared Methods

All substudies draw on the same clinical cohort and share a common set of intake and outcome variables. The shared dataset comprises (a) pre-therapy intake data, including sociodemographics, standardized psychometric questionnaires, and question-specific open-ended patient narratives, and (b) longitudinal clinical measures collected repeatedly during and after treatment. Textual analyses are based exclusively on pre-therapy narratives, while psychometric and clinician-rated measures are used as cross-sectional outcomes, covariates, or longitudinal endpoints depending on the substudy.

The following sections describe the shared dataset, preprocessing pipeline, and measurement instruments used across all substudies.

## Measures

### Sociodemographic and context measures

Sociodemographic information included age, sex, marital and relationship status, general educational attainment, vocational qualification, and current work ability. Contextual variables captured prior psychological or psychiatric treatment and the manner in which therapy ended (e.g., regular completion, dropout).

### Responses from open-ended questions before therapy

At the start of therapy, patients complete two separate questionnaires designed to assess key aspects of their mental health concerns, functional impairments, and expectations for treatment. Questions 1–9 come from the first questionnaire (*Fragebogen zur Lebensgeschichte*), and questions 10–12 come from the second (*Eingangsfragebogen*). The questions include:

1.  **Problem development:** ‘Briefly describe how the problems for which you are seeking treatment have developed over time.’ (german original question: „Beschreiben Sie kurz, wie sich Ihre Probleme, wegen derer Sie eine Behandlung aufsuchen, im Laufe der Zeit entwickelt haben.")
2.  **Extra stressors:** ‘What causes you stress in addition to your everyday problems (e.g. finances, housing situation)?’ (german original question: „Was macht Ihnen zusätzlich zu Ihren Problemen im Alltag Stress (z. B. Finanzen, Wohnsituation)?“)
3.  **Pre-onset changes:** ‘Did something special change in your life before the onset of your symptoms? (e.g. death of an important person, divorce or separation, change in work situation or income, addition to the family)’ (german original question: „Hat sich vor dem Beginn Ihrer Beschwerden etwas Besonderes in Ihrem Leben verändert? (z. B. Tod einer wichtigen Bezugsperson, Scheidung oder Trennung, Veränderung der Arbeitssituation oder des Einkommens, Familienzuwachs)“)
4.  **Event connection:** ‘Do you see a connection between the event(s) and the development of your problems?’ (german original question: „Sehen Sie einen Zusammenhang zwischen dem Ereignis/den Ereignissen und der Entwicklung Ihrer Probleme?“)
5.  **Physical symptoms:** ‘Are there any physical side effects when your problems occur?’ (german original question: „Gibt es körperliche Begleiterscheinungen, wenn Ihre Probleme auftreten?“)
6.  **Problem causes:** ‘What do you think are the causes of your problems?’ (german original question: „Welche Ursachen sehen Sie für Ihre Probleme?“)
7.  **Expected improvements:** ‘What would improve in your life if you no longer had your problems?’ (german original question: „Was würde sich in Ihrem Leben verbessern, wenn Sie ihre Probleme nicht mehr hätten?“)
8.  **Environment response:** ‘How does your environment (partner, family, friends, work colleagues) react to your problems?’ (german original question: „Wie reagiert Ihre Umwelt (Partner:in, Familie, Freund:innen, Arbeitskolleg:innen) auf die Probleme?")
9.  **No change required:** ‘What should not change under any circumstances as a result of the therapy?’ (german original question: „Was sollte sich durch die Therapie auf keinen Fall verändern?“)
10. **Problem description:** ‘Finally, please describe in your own words the problems for which you would like treatment.’ (german original question: „Beschreiben Sie zum Abschluss bitte noch einmal in eigenen Worten Ihre Probleme, deretwegen Sie eine Behandlung wünschen.“)
11. **Impacted life areas:** ‘In which areas of your life do these problems limit you (e.g. job, relationship)?’ (german original question: „In welchen Lebensbereichen schränken Sie diese Probleme ein (z. B. Beruf, Partnerschaft)?“)
12. **Therapy goals:** ‘What would you like to achieve for yourself in therapy?’ (german original question: „Was möchten Sie in der Therapie für sich erreichen?“)

### Psychometric measures

Clinical and psychometric variables were retrieved from the FBZ database and included diagnostic information, self-report symptom measures, therapist- and patient-rated outcome measures, positive mental health indicators, and therapeutic process variables. Diagnoses were coded according to DSM-5 and ICD-10 criteria. Symptom severity and treatment outcomes were assessed using a combination of standardized self-report questionnaires and clinician-rated instruments administered at different points during treatment.

#### Diagnosis

Diagnosis at the outpatient clinic is conducted using structured clinical interviews. These typically take place before therapy begins, usually at the fourth therapist–patient contact. The interview used is the Diagnostic Interview for Mental Disorders [@margraf2021], which covers the most frequent DSM-5 disorders encountered in outpatient therapy settings.

#### Beck-Depression-Inventory II

Depressive symptoms were assessed using the *Beck Depression Inventory–II* (BDI-II; [@Beck1996]), a widely used self-report questionnaire measuring the severity of depressive symptomatology over the past two weeks.

#### Depression Anxiety Stress Scale 42

Depressive symptoms, anxiety symptoms, and general psychological distress were assessed using the Depression Anxiety Stress Scale–42*Depression Anxiety Stress Scale–42* (DASS-42[@Lovibond1995]), which consists of 42 items measuring symptoms of depression, anxiety, and stress on a 4-point likert scale.

#### Brief Symptom Inventory

Overall psychopathological symptom burden was measured using the *Brief Symptom Inventory* (BSI; [@book]), the short form of the Symptom Checklist-90-Revised (SCL-90-R; Derogatis). The BSI consists of 53 items rated on a 5-point Likert scale ranging from 0 (“not at all”) to 4 (“extremely”). Responses to 49 items are assigned to nine primary symptom dimensions, while four items are evaluated separately. These symptom dimensions are summarized into three global indices: the *Global Severity Index* (GSI), reflecting overall psychological distress; the *Positive Symptom Distress Index* (PSDI), indicating symptom intensity; and the *Positive Symptom Total* (PST), representing the number of reported symptoms.

#### Positive Mental Health Scale

Positive mental health (PMH) was assessed with the nine-item *PMH scale* [@lukat2016]. Responses are given on a 4-point Likert scale from 0 (disagree) to 3 (agree). Item scores are summed to yield a total score ranging from 0 to 27, with higher scores reflecting greater PMH. The scale has been validated as a unidimensional measure with excellent internal consistency (Cronbach’s α = .93), good test–retest reliability (Pearson r = .74–.81), and evidence of scalar invariance across samples and over time [@lukat2016]. Furthermore, it shows strong convergent and discriminant validity and is sensitive to therapeutic change across diverse populations [@lukat2016].

#### Childhood Trauma Questionnaire

Early adverse experiences were assessed using the *Childhood Trauma Questionnaire* (CTQ) [@bernstein2003], a widely used self-report instrument for the retrospective assessment of childhood maltreatment. The CTQ measures five domains of adverse experiences: emotional abuse, physical abuse, sexual abuse, emotional neglect, and physical neglect. Items are rated on a 5-point Likert scale ranging from 1 ("not at all") to 5 ("very often"), with higher scores indicating greater exposure to maltreatment. The German version of the CTQ has demonstrated good psychometric properties, including satisfactory reliability and validity in clinical samples [@wingenfeld2010].

#### Clinical Global Impression

Clinician-rated symptom severity and improvement were assessed using the *Clinical Global Impression* (CGI) scales.

##### Severity Scale

The *CGI-Severity* scale evaluates the clinician’s global impression of the patient’s current level of mental illness, based on their total clinical experience with this population. The item asks: *“Considering your total clinical experience with this particular population, how mentally ill is the patient at this time?”*

##### Improvement Scale

Treatment-related change was assessed using the *CGI-Improvement* scale. Both patients and therapists rated overall improvement relative to the beginning of therapy, regardless of whether the change was attributed entirely to treatment. Patient and therapist versions differ only in perspective but use equivalent response formats.

### Global Improvement

Global therapy outcome was assessed using a six-point global success rating based on two items measuring perceived benefit and satisfaction with therapy [@michalak2003]. These items were completed by both patients and therapists and capture a retrospective evaluation of treatment success. The items assess (1) the extent to which expectations toward therapy have been fulfilled and (2) the overall perceived benefit of therapy. Responses are given on a 6-point Likert scale ranging from 1 (“on the contrary / rather harmful”) to 6 (“completely / very helpful”).

### Goal Attainment Scale

Individualized treatment outcomes were assessed using a goal attainment measure inspired by the Goal Attainment Scaling approach [@kiresuk1968]. At the beginning of therapy, patients and therapists collaboratively define individualized treatment goals. During the course and at the end of therapy, patients and therapists retrospectively evaluated the extent to which each of the predefined goals had been achieved.

Goal attainment was rated on a standardized six-point numerical scale ranging from deterioration relative to the initial goal state (-1 = moved away from the goal) to full goal attainment (4 = goal achieved). The scale reflects patients’ subjective assessment of goal progress, with intermediate categories indicating partial progress toward the respective goal.

For each patient, an overall goal attainment score was computed as the mean rating across all individually defined goals, representing the average subjective level of goal progress at the end of therapy.

## Measurement time points

| Timepoint | DU-DI | DU-Prä | KZT1-DU4 | KZT1-DUPost | KZT2-DUPost | LZT1-DUPost | LZT2-DUPost | Kat6 |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| **Explanation** | Pre-therapy, 4th contact | Pre-therapy, 6th contact | 4th therapy session | 12th therapy session | 24th therapy session | 45th therapy session | 60th therapy session | 6 month after therapy |
| **Diagnosis** | X |  |  |  |  |  |  |  |
| **Demographics** |  | X |  |  |  |  |  |  |
| **BSI** | X |  | X | X | X | X | X | X |
| **BDI-II** |  | X | X | X | X | X | X | X |
| **DASS-42** |  | X | X | X | X | X | X | X |
| **PMH** |  | X | X | X | X | X | X | X |
| **CTQ** |  | X |  |  |  |  |  |  |
| **CGI-S** |  | X |  |  |  |  |  |  |
| **CGI-I** |  |  |  | X | X | X | X | X |
| **Glob-Pt** |  |  |  | X | X | X | X | X |
| **GAS** |  |  |  | X | X | X | X | X |

## Preprocessing

To streamline data collection, an automated transcription pipeline was implemented: The handwritten text data is first recorded by trained employees of the FBZ adult outpatient clinic using a mobile audio recording device. Identifying features (e.g. names, dates of birth, location details) were replaced by placeholders during recording. The transcription was carried out on local hardware using the open source tool Whisper Large v2 (<https://github.com/openai/whisper>), a state-of-the-art speech-to-text model [@Radford2022]. Each recording begins with a structured introduction, including a patient identification code, followed by responses to predefined questions. The transcription pipeline automatically processes all audio recordings, extracts the patient codes, and identifies responses to key questions.

As an additional data correction step, the exported transcription table was screened for incomplete entries. Records with missing patient identification codes or without any extracted text were automatically flagged, exported for manual correction, and subsequently re-imported and merged back into the original dataset. The corrected dataset was then used for downstream analyses.

## Shared analytic framework

All substudies in this project are based on the same clinical cohort and share a common analytic foundation. Specific operationalizations of text inputs, outcomes, and analytic models differ across substudies and are specified in the corresponding substudy sections below.

**Data scope and unit of analysis**. Across all substudies, analyses focus on pre-therapy patient narratives collected during intake. Language-based analyses are temporally ordered such that patient narratives precede all clinical outcomes of interest. Outcomes may be assessed cross-sectionally (at intake) or longitudinally (during or after therapy), depending on the substudy. The unit of analysis is the individual patient. Textual data collected during therapy are not used as predictors in any analysis.

**Descriptive characterization of open-ended responses.** As a shared descriptive foundation, responses to each open-ended prompt are characterized with respect to engagement and heterogeneity. Descriptive statistics include response length and an entropy-based lexical diversity index [@shannon1948mathematical], computed across pooled responses per question. This characterization provides a common empirical basis for interpreting prompt-specific response patterns across substudies.

Because the open-ended questions were administered in two questionnaire blocks, analyses are restricted to cases in which the respective questionnaire was present. Within these blocks, item-level nonresponse is summarized descriptively and interpreted as potentially informative of selective responding.

::: {pagebreak}
:::

# Preliminary results

## Descriptive statistics of textual data

Descriptive analyses revealed substantial heterogeneity across questions in response rates, length, and lexical diversity. Holistic questions such as problem description (q10) and therapy goals (q12) showed low missingness, longer median response lengths, and high lexical diversity, indicating that patients readily produced extended and heterogeneous narratives when asked to reflect broadly on their difficulties or desired changes. In contrast, prompts targeting causal connections (q4) or constraints (q11) frequently elicited short or missing responses and exhibited lower diversity, consistent with more constrained or confirmatory response formats. Notably, lexical diversity varied independently of response length, suggesting that some prompts elicited shared narrative scripts despite moderate verbosity (e.g., q7 Expected Improvements).

```{r}
#| include: false

# ------------------------------------------------------------
# Load processed working dataset
# ------------------------------------------------------------
# Contains one column per open-ended question (txt1_..., txt2_..., etc.)
working_data <- readRDS("/Volumes/meta_data/processed/working_data/merged_data.rds")

# ------------------------------------------------------------
# Load required packages
# ------------------------------------------------------------
library(tidyverse)   # data wrangling
library(stringi)     # word counts
library(knitr)       # table output
library(tidytext)    # tokenization for lexical diversity

# ------------------------------------------------------------
# Identify text columns corresponding to open-ended questions
# ------------------------------------------------------------
# Assumes columns are named like "txt1_problem", "txt2_stressors", etc.
text_cols <- names(working_data)[str_detect(names(working_data), "^txt\\d+_")]

# ------------------------------------------------------------
# Reshape data to long format: one row per person × question
# ------------------------------------------------------------
# This makes it easy to compute question-level descriptives
working_data_long <- working_data |> 
  pivot_longer(
    cols = all_of(text_cols),
    names_to = "question_raw",
    values_to = "text"
  ) %>%
  mutate(
    # Extract numeric question ID (1–12) from column name
    question_id = str_extract(question_raw, "\\d+") |> as.integer(),
    
    # Extract human-readable question label from column name
    question_label = str_remove(question_raw, "^txt\\d+_"),
    
    # Normalize whitespace in text
    text = str_squish(text),
    
    # Flag missing or empty responses
    is_missing = is.na(text) | text == ""
  )

# ------------------------------------------------------------
# Compute basic per-response text features
# ------------------------------------------------------------
# These are *response-level* quantities, later summarized by question
working_data_long <- working_data_long |> 
  mutate(
    n_chars = ifelse(is_missing, NA_integer_, nchar(text)),            # character count
    n_words = ifelse(is_missing, NA_integer_, stri_count_words(text)), # word count
    is_very_short = !is_missing & n_words <= 3                         # list-like / minimal responses
  )

# ------------------------------------------------------------
# Question-level descriptive statistics (engagement & length)
# ------------------------------------------------------------
# These summarize HOW MUCH people write, not WHAT they write
qc_by_question <- working_data_long |> 
  group_by(question_id, question_label) |> 
  summarise(
    n_total = n(),                                  # total number of responses
    missing_pct = mean(is_missing) * 100,           # nonresponse rate
    very_short_pct = mean(is_very_short) * 100,     # minimal responses
    median_words = median(n_words, na.rm = TRUE),   # typical response length
    p10_words = quantile(n_words, 0.10, na.rm = TRUE),
    p90_words = quantile(n_words, 0.90, na.rm = TRUE),
    .groups = "drop"
  )

# ------------------------------------------------------------
# Human-readable question labels for tables
# ------------------------------------------------------------
question_labels <- c(
  "Problem development",
  "Extra stressors",
  "Pre-onset changes",
  "Event connection",
  "Physical symptoms",
  "Problem causes",
  "Expected improvements",
  "Environment response",
  "No change required",
  "Problem description",
  "Impacted life areas",
  "Therapy goals"
)

# ------------------------------------------------------------
# Tokenize responses for lexical diversity analysis
# ------------------------------------------------------------
# Goal: quantify how diverse the vocabulary is ACROSS RESPONDENTS per question
# (not within individual responses)
tokens_by_question <- working_data_long |> 
  filter(!is_missing) |>                     # exclude missing responses
  unnest_tokens(word, text, token = "words") |>  # split into word tokens
  filter(!str_detect(word, "^\\d+$"))        # remove pure numbers

# ------------------------------------------------------------
# Compute Shannon entropy and diversity index per question
# ------------------------------------------------------------
# Entropy reflects dispersion of word usage across all responses
# Diversity index = 2^H = "effective number of equally frequent word types"
diversity_by_question <- tokens_by_question |> 
  count(question_id, question_label, word) |>   # word frequencies per question
  group_by(question_id, question_label) |> 
  mutate(p = n / sum(n)) |>                     # word probabilities
  summarise(
    shannon_entropy = -sum(p * log2(p)),        # Shannon entropy
    diversity_index = 2 ^ shannon_entropy,      # effective vocabulary size
    total_tokens = sum(n),                      # total words (for context)
    vocab_size = n_distinct(word),              # raw vocabulary size
    .groups = "drop"
  )

# ------------------------------------------------------------
# Combine all descriptives into final question-level table
# ------------------------------------------------------------
# This table triangulates:
# - engagement (missing, very short)
# - quantity (word counts)
# - dispersion (lexical diversity)
qc_table <- qc_by_question |>
  mutate(
    Question = factor(question_id, levels = 1:12, labels = question_labels)
  ) |>
  left_join(
    diversity_by_question,
    by = c("question_id", "question_label")
  ) |>
  select(
    Question,
    n_total,
    missing_pct,
    very_short_pct,
    median_words,
    p10_words,
    p90_words,
    diversity_index
  )

```

```{r}
#| echo: false

qc_table |>
  kable(
    caption = "Table X. Descriptive statistics of open-ended responses by question.",
    align = "lccccccc",
    booktabs = TRUE
  )
```

## Descriptive statistics of clinical data

```{r}
#| include: false

library(kableExtra)

# --- 1) Build long DSMV data (one row per case x slot) ----
dsmv_cols <- grep("^dsmv_diagnosis_[1-7](_|$)", names(working_data), value = TRUE)

first_non_empty <- function(x) {
  x <- x[!is.na(x) & x != ""]
  if (length(x) == 0) NA_character_ else x[1]
}

dsmv_long <- working_data %>%
  select(case_nr, all_of(dsmv_cols)) %>%
  mutate(across(-case_nr, as.character)) %>%
  pivot_longer(
    cols = -case_nr,
    names_to = c("slot", "field"),
    names_pattern = "^dsmv_diagnosis_([1-7])(?:_(.*))?$",
    values_to = "value"
  ) %>%
  mutate(field = if_else(is.na(field) | field == "", "diagnosis", field)) %>%
  pivot_wider(
    names_from  = field,
    values_from = value,
    values_fn   = first_non_empty
  ) %>%
  filter(!is.na(diagnosis) & diagnosis != "")

# --- 2) translate german diagnostic labels to english ----

lookup <- read_delim("/Volumes/meta_data/processed/working_data/diagnosis_lookup_de_en.csv", delim = ";", show_col_types = FALSE)

dsmv_long_en <- dsmv_long %>%
  left_join(lookup, by = c("diagnosis" = "diagnosis_de")) %>%
  mutate(diagnosis = coalesce(diagnosis_en, diagnosis)) %>%  # fallback if missing
  select(-diagnosis_en)

# --- 3) Counts: all vs primary (by type label) ----
all_diag <- dsmv_long_en %>%
  count(diagnosis, name = "n_all")

primary_diag <- dsmv_long_en %>%
  filter(type == "primary diagnosis") %>%
  count(diagnosis, name = "n_primary")

# --- 4) Combine + fill NAs ----
diag_summary <- full_join(all_diag, primary_diag, by = "diagnosis") %>%
  mutate(
    n_all = replace_na(n_all, 0L),
    n_primary = replace_na(n_primary, 0L)
  )

# --- 5) Collapse Major Depression subtypes (store the result!) ----
diag_collapsed <- diag_summary %>%
  mutate(
    diagnosis = if_else(
      grepl("^Major Depressive", diagnosis),
      "Major Depression (all subtypes)",
      diagnosis
    )
  ) %>%
  group_by(diagnosis) %>%
  summarise(
    n_all = sum(n_all),
    n_primary = sum(n_primary),
    .groups = "drop"
  )

# --- 6) Keep >=10, and add "Other (<10)" from the collapsed table ----
diag_main <- diag_collapsed %>%
  filter(n_all >= 10) %>%
  arrange(desc(n_all))

other_row <- diag_collapsed %>%
  filter(n_all < 10) %>%
  summarise(
    diagnosis = "Other diagnoses (<10)",
    n_all = sum(n_all),
    n_primary = sum(n_primary)
  )

diag_table_final <- bind_rows(diag_main, other_row)
```

```{r}
#| echo: false

diag_table_final %>%
  rename(
    `DSM-V diagnosis` = diagnosis,
    `All diagnoses (n)` = n_all,
    `Primary diagnoses (n)` = n_primary
  ) %>%
  kable(
    align = c("l", "c", "c"),
    booktabs = TRUE,
    caption = "DSM-5 diagnoses in the sample (absolute frequencies)"
  ) %>%
  kable_styling(full_width = FALSE, position = "left") %>%
  footnote(general = "Diagnoses were collapsed for display. Only diagnoses with ≥10 occurrences are shown individually; remaining diagnoses are summarized as 'Other diagnoses (<10)'.")
```

::: {pagebreak}
:::

# Substudies

This project comprises multiple complementary substudies that share a common dataset and preprocessing pipeline but address distinct research questions. To avoid analytic flexibility and to maximize interpretability, each substudy pre-specifies its text inputs, outcomes, and evaluation strategy.

## Substudy 1: Language as clinical assessment (cross-sectional)

**Core research question.** Does patients’ pre-therapy language provide clinically meaningful information about symptom severity and functioning, and does it add predictive value beyond established self-report questionnaires? Additionally, to what extent is this information captured by large language model (LLM)–based representations compared with simpler linguistic features?

**Conceptual contribution.** This substudy conceptualizes patient-generated language as an independent assessment modality rather than a proxy for existing symptom scales. While prior work has demonstrated correlations between language and self-reported symptoms, few studies have tested whether language captures incremental clinical information beyond standardized questionnaires, particularly for clinician-rated outcomes.

**Text inputs.** Q1 Problem developtment, Q10 *Problem description,* Q12 *Therapy goals*

**Outcomes.**

-   Convergent validity outcomes: Cross-sectional self-report symptom (BDI-II, BSI/GSI, DASS-42) and well-being (PMH) measures

-   Incremental validity outcomes: Clinician-rated symptom severity (CGI-Severity).

**Models & evaluation.** Convergent validity with self-report symptom measures (BDI-II, BSI/GSI, DASS-42) will be evaluated by estimating the proportion of symptom variance explained by language-based representations. Incremental validity will be tested by assessing whether language explains additional variance in clinician-rated symptom severity (CGI-Severity) beyond self-report questionnaires.

The Sequential Evaluation with Model Pre-registration [@kjell_ganesan_boyd_oltmanns_rivero_feltman_carr_luft_kotov_schwartz_2024] framework will be implemented to ensure robust model development, mitigating overfitting and enabling unbiased performance evaluation. Additionally, evaluating models on prospective data will simulate real-world clinical deployment by assessing performance on new, unseen patient data.

During the model development phase, preprocessing pipelines will be finalized, and exploratory models developed using advanced cross-validation techniques. Contextual embeddings derived from pretrained LLMs will be linked to clinical outcomes using state-of-the-art prediction models, including ridge regression [@Hoerl1970], lasso regression [@10.1111/j.2517-6161.1996.tb02080.x], and random forests [@598994]. The final pipelines will be pre-registered for evaluation (e.g., <https://aspredicted.org/>). In the evaluation phase, pre-registered models will be tested on held-out datasets, enabling unbiased performance assessments.

![The step-by-step process of patient narrative analysis, from preprocessing to data evaluation.](reports/Abbildung_Gabriel_eng.png){#figure_1 width="100%"}

## Substudy 2: Patient narratives and themes

**Core research question.** How do patients conceptualize the their mental health problems, as well as anticipated improvements, and how do these narrative patterns vary across individuals and clinical groups?

**Conceptual contribution.** This substudy emphasizes interpretation and theory generation, using patient language to uncover recurring narrative themes. By analyzing these themes, this substudy provides insight into how narrative content and response patterns relate to diagnostic categories, baseline severity or sociodemographic (e.g. age, gender) and anamnestic (e.g. childhood trauma) characteristics.

**Text inputs.** Q1 *Problem development*, Q2 *Extra stressors*, Q3 *Pre-onset changes*, Q6 *Problem causes*, Q7 *Expected improvements*, Q8 *Environment response*. Question-wise modeling.

**Outcomes.** No single predictive outcome is specified, as the primary aim is interpretive and theory-generating rather than predictive.

**Models & evaluation.** Topic modeling / embedding-based clustering; interpretive labeling; comparison of topic prevalence across patient groups.

## Substudy 3: Predicting treatment response and goal attainment (longitudinal prediction)

**Core research question.** Can patients’ pre-therapy language predict clinically meaningful treatment outcomes beyond baseline symptom severity and demographic characteristics?

**Conceptual contribution.** This substudy extends language-based mental health assessment from cross-sectional validity to prognostic utility. While prior work has primarily examined whether language reflects current symptom severity, far less is known about whether pre-therapy narratives encode information relevant for future treatment response, such as motivation, goal clarity, perceived agency, or narrative coherence. By evaluating the ability of pre-therapy language to predict longitudinal outcomes above and beyond baseline symptom and well-being measures, this substudy tests whether patient-generated text captures clinically actionable signals that are not accessible through standard intake questionnaires alone.

**Text inputs.** Primary: Q10, Q12, Q7, Q1; Secondary: concatenation models.

**Outcomes.**

-   Clinician-rated improvement (CGI-Improvement) at 6 months-follow-up assessment.

-   Global therapy outcome ratings (patient- and therapist-reported) at 6 months-follow-up assessment.

-   Goal Attainment Scale at 6 months-follow-up assessment.

-   Pre-Post differences in symptom (BDI-II, BSI/GSI, DASS-42) and well-being (PMH) scales .

**Models & evaluation.**

-   Baseline prognostic models: Demographic variables and baseline symptom severity and well-being (e.g., BDI-II, BSI/GSI, PMH, CGI-Severity).

-   Language-augmented models: Baseline predictors plus language representations derived from pre-therapy text.

## Substudy 4: Prompt-based LLM rubrics for interpretable language assessment

**Core research question.** Can prompt-based large language models (LLMs) reliably extract interpretable, clinically meaningful rubric scores from pre-therapy narratives, and do these rubric-based language measures add incremental value beyond (a) standardized questionnaires and (b) embedding-based language representations?
